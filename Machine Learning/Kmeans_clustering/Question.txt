Implement K-Means clustering/ hierarchical clustering on 
sales_data_sample.csv  dataset. Determine the number of clusters using the 
elbow method.

Pandas - Data loading, cleaning, analysis.
numpy - Used for mathematical and numerical operations on arrays.

seaborn - Used for statistical data visualization.Itâ€™s built on top of Matplotlib but provides prettier and more advanced charts like heatmaps, boxplots, and pairplots.

matplotlib - Used for basic plotting and visualization â€” like line graphs, bar charts, scatter plots, and histograms.

ğŸ”¹ KMeans

Purpose:
Used for clustering, i.e., grouping similar data points together without labels (unsupervised learning).
It divides the dataset into a predefined number of clusters (say, 3 or 5).
Each data point is assigned to the nearest cluster center (centroid).

ğŸ”¹from sklearn.decomposition import PCA
PCA stands for Principal Component Analysis â€” a dimensionality reduction technique.

Purpose:
To reduce the number of features (columns) in your dataset while keeping the most important information.


| Component            | Purpose                                  |
| -------------------- | ---------------------------------------- |
| `pd.read_csv()`      | Reads a CSV file into a Pandas DataFrame |
| `sep=","`            | Columns are separated by commas          |
| `encoding='Latin-1'` | Handles special characters safely        |
| `df`                 | Stores the loaded dataset                |

ğŸ‘‰ df.shape #used to find out the size (dimensions) of your dataset (rows , columns).

ğŸ‘‰ df.describe() gives you a quick statistical summary of your dataset â€” like average, range, and spread â€” to help you understand your data better before analysis.

ğŸ‘‰ df.info() gives a quick summary of your dataset â€” how many rows, what columns exist, their data types, and whether any data is missing.

ğŸ‘‰ df.isnull() â†’ Returns a DataFrame of the same shape as df,
with True where a value is missing and False where it is not.

.sum() â†’ Adds up all the True values column-wise
(since True = 1 and False = 0 in Python).

So together:
df.isnull().sum() = Count of missing values per column.

ğŸ‘‰pd.get_dummies() converts categorical values into binary (0/1) columns â€”
a process called One-Hot Encoding.

ğŸ‘‰df = pd.concat([df,productline,Dealsize], axis = 1)
This command adds the new one-hot encoded columns (productline and Dealsize) back into your main DataFrame df.

So now your df will contain both:
The original columns, and
The new dummy (0/1) columns created from categorical data.

ğŸ‘‰ distortions = []
Creates an empty list to store the inertia (distortion) values for each KMeans model.
This loop runs KMeans for different numbers of clusters (1â€“9), measures how well each one fits (using inertia), and stores those values so you can find the best number of clusters using the Elbow Method.

ğŸ§© Graph Shown:

ğŸ“ˆ â€œThe Elbow Method showing the optimal kâ€

ğŸ”¹ Purpose of the Graph:

The goal of this graph is to find the optimal number of clusters (k) for your dataset when using K-Means clustering.

In K-Means, you must tell the algorithm how many clusters (k) you want to create.
The Elbow Method helps you decide that number visually.

ğŸ”¹ Whatâ€™s on the Axes:
Axis	Represents	Meaning
X-axis (k)	Number of clusters	You tested values of k from 1 to 9
Y-axis (Distortion / Inertia)	Within-Cluster Sum of Squares (WCSS)	Measures how close data points are to their cluster centers

ğŸ”¹ How to Read the Graph:

Each point on the graph shows the total â€œdistortionâ€ (error) for a specific number of clusters (k).
As you increase k (add more clusters):
The distortion decreases because clusters fit the data better.
But after a certain point, the improvement becomes very small â€” thatâ€™s the â€œelbowâ€.

ğŸ”¹ What the â€œElbowâ€ Means:

The â€œelbow pointâ€ (where the curve starts to bend and flatten) indicates the optimal number of clusters.

Adding more clusters after this point gives diminishing returns â€” it doesnâ€™t improve results much.

âœ… In Your Graph:
Looking at your curve:
Thereâ€™s a sharp drop from k = 1 â†’ 3.
After k = 3, the curve starts to flatten.

ğŸ‘‰ This means the optimal number of clusters (k) is likely around 3.

ğŸ”¹ Purpose of the Graph: 

This scatter plot visually shows how your data points are distributed in 2D space after applying PCA.

Since your original dataset likely had many columns (features), you canâ€™t visualize it easily.
PCA compresses that information into 2 main components â€” the two directions that capture the most variance (spread) in the data.

ğŸ”¹ Axes Meaning:
Axis	Meaning
X-axis â†’ PCA1	The first principal component â€” the direction capturing the maximum variance (most important pattern in data).
Y-axis â†’ PCA2	The second principal component, orthogonal (independent) from the first â€” it captures the next most significant variation.

Last Graph :-
The K-Means algorithm analyzed your data and grouped similar points into 3 clusters (since you chose k = 3 earlier).

The different colors show which data points belong to which cluster.

The black X marks are the cluster centroids â€” each clusterâ€™s â€œcenter of gravityâ€.

The distance between clusters shows how distinct they are:

    * If clusters are far apart, they are well separated.
    * If clusters overlap, some data points are similar between groups.

This scatter plot shows how your dataset was divided into 3 groups by the K-Means algorithm. Each color represents a cluster, and the black X marks are the centers of those clusters. PCA was used to reduce the data to 2D for easy visualization of these groups.
